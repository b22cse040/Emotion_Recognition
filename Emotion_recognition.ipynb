{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV6ifbzaJuxr"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ananthu017/emotion-detection-fer\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_path='/kaggle/input/emotion-detection-fer'\n",
        "data=os.listdir(data_path)\n",
        "print(f\"Total folders in this Dataset: {len(data)}\")\n",
        "print(f\"Files in this Dataset: {data}\")"
      ],
      "metadata": {
        "id": "MtUSWEKhJ-d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in data:\n",
        "    split_path = os.path.join(data_path, split)\n",
        "    classes = os.listdir(split_path)\n",
        "    print(f\"{split} classes:\", classes)\n",
        "\n",
        "    counts = {cls: len(os.listdir(os.path.join(split_path, cls))) for cls in classes}\n",
        "    print(f\"{split} samples per class:\", counts)"
      ],
      "metadata": {
        "id": "tzu2RTgyKGYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "train_dir = os.path.join(data_path, \"train\")\n",
        "\n",
        "example_path = glob(os.path.join(data_path, \"train\", \"*\", \"*.png\"))\n",
        "print(\"Total training images:\", len(example_path))\n",
        "\n",
        "# 7 Random Images (1 images from each Class)\n",
        "fig, axes = plt.subplots(1, 7, figsize=(22,8))\n",
        "for ax in axes:\n",
        "    img_path = random.choice(example_path)\n",
        "    label = img_path.split(os.sep)[-2]\n",
        "    img = Image.open(img_path).convert(\"L\")  # grayscale\n",
        "    ax.imshow(img, cmap=\"gray\")\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n",
        "print(\"Image size (W x H):\", img.size)"
      ],
      "metadata": {
        "id": "XUsBrN7SKJb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "o4G0zWxTK48V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ],
      "metadata": {
        "id": "7k23EUrqK4a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((48,48)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((48,48)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Train Dataset\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_path, \"train\"), transform=train_tf)\n",
        "print(f\"Train set size: {len(train_dataset)}\")\n",
        "\n",
        "# Test Dataset\n",
        "test_dataset  = datasets.ImageFolder(os.path.join(data_path, \"test\"), transform=eval_tf)\n",
        "print(f\"Test set size: {len(test_dataset)}\")\n",
        "\n",
        "# Class names\n",
        "class_names = train_dataset.classes"
      ],
      "metadata": {
        "id": "XRQo37PTLSqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=64\n",
        "\n",
        "class OneHotImageFolder(Dataset):\n",
        "    def __init__(self, image_folder_dataset, num_classes):\n",
        "        self.dataset = image_folder_dataset\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        # Convert to one-hot\n",
        "        one_hot_label = torch.nn.functional.one_hot(torch.tensor(label), num_classes=self.num_classes).float()\n",
        "        return image, one_hot_label\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 7\n",
        "\n",
        "# Wrap train and test datasets\n",
        "train_dataset_oh = OneHotImageFolder(train_dataset, num_classes=num_classes)\n",
        "test_dataset_oh  = OneHotImageFolder(test_dataset, num_classes=num_classes)\n",
        "\n",
        "# Data Loaders\n",
        "train_loader = DataLoader(train_dataset_oh, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset_oh, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "ZVxjWZhGLttt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "UBDgZooZM91-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EmotionManipulator(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_p=0.3):\n",
        "        super(EmotionManipulator, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # --- Block 1 ---\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),   # 48x48 -> 48x48\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # 48x48 -> 48x48\n",
        "            nn.ReLU(),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2),                              # 48x48 -> 24x24\n",
        "            nn.Dropout(p=dropout_p),\n",
        "\n",
        "            # --- Block 2 ---\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 24x24 -> 24x24\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 24x24 -> 24x24\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),                              # 24x24 -> 12x12\n",
        "            nn.Dropout(p=dropout_p),\n",
        "\n",
        "            # --- Block 3 ---\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # 12x12 -> 12x12\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),# 12x12 -> 12x12\n",
        "            nn.ReLU(),\n",
        "            # nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2),                              # 12x12 -> 6x6\n",
        "            nn.Dropout(p=dropout_p),\n",
        "\n",
        "            # --- Block 4 ---\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# 6x6 -> 6x6\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),# 6x6 -> 6x6\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(2),                              # 6x6 -> 3x3\n",
        "            nn.Dropout(p=dropout_p)\n",
        "        )\n",
        "\n",
        "        # --- Fully Connected Layers ---\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 3 * 3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "num_classes = 7\n",
        "model = EmotionManipulator(num_classes=num_classes)\n",
        "dummy_input = torch.randn(4, 1, 48, 48)\n",
        "output = model(dummy_input)\n",
        "print(output.shape)  # torch.Size([4, 7])\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "1wsFN1c_M9Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class EmotionManipulatorResnet(nn.Module):\n",
        "    def __init__(self, num_classes=7, backbone='resnet18', pretrained=True, dropout_p=0.4, fine_tune_layers=1):\n",
        "        super(EmotionManipulatorResnet, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet\n",
        "        resnet = getattr(models, backbone)(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "\n",
        "        # Modify first conv layer â†’ grayscale input\n",
        "        old_weights = resnet.conv1.weight.data\n",
        "        new_weights = old_weights.mean(dim=1, keepdim=True)\n",
        "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        resnet.conv1.weight.data = new_weights\n",
        "\n",
        "        # Extract all layers except final FC\n",
        "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        # Freeze most of backbone (optionally unfreeze top few layers)\n",
        "        for name, param in self.feature_extractor.named_parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if fine_tune_layers > 0:\n",
        "            # Unfreeze last 'fine_tune_layers' blocks of layer4\n",
        "            for name, param in list(self.feature_extractor[-1].named_parameters())[-fine_tune_layers:]:\n",
        "                param.requires_grad = True\n",
        "\n",
        "        # Richer classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p * 0.8),\n",
        "\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.set_grad_enabled(any(p.requires_grad for p in self.feature_extractor.parameters())):\n",
        "            x = self.feature_extractor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "num_classes = 7\n",
        "model = EmotionManipulatorResnet(num_classes=num_classes, fine_tune_layers=2)\n",
        "dummy_input = torch.randn(4, 1, 48, 48)\n",
        "output = model(dummy_input)\n",
        "print(output.shape)  # torch.Size([4, 7])\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ],
      "metadata": {
        "id": "ARaTQpS_WejW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout_p=0.3, use_batchnorm=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=False),\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "        self.shortcut = (\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.conv_block(x)\n",
        "        out = self.bn(out)\n",
        "        out = out + identity\n",
        "        out = torch.relu(out)\n",
        "        out = self.pool(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class EmotionManipulatorResidual(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_p=0.3):\n",
        "        super(EmotionManipulatorResidual, self).__init__()\n",
        "\n",
        "        # --- Residual Feature Extractor ---\n",
        "        self.features = nn.Sequential(\n",
        "            ResidualBlock(1, 32, dropout_p, use_batchnorm=False),   # 48x48 -> 24x24\n",
        "            ResidualBlock(32, 64, dropout_p, use_batchnorm=True),   # 24x24 -> 12x12\n",
        "            ResidualBlock(64, 128, dropout_p, use_batchnorm=False), # 12x12 -> 6x6\n",
        "            ResidualBlock(128, 256, dropout_p, use_batchnorm=True), # 6x6 -> 3x3\n",
        "        )\n",
        "\n",
        "        # --- Classifier ---\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 3 * 3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = EmotionManipulatorResidual(num_classes=7)\n",
        "dummy_input = torch.randn(4, 1, 48, 48)\n",
        "output = model(dummy_input)\n",
        "\n",
        "print(output.shape)  # torch.Size([4, 7])\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters (Residual): {total_params:,}\")"
      ],
      "metadata": {
        "id": "bhM0HBPwF-Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # shape: (batch, num_classes)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Accuracy calculation: compare predicted class with one-hot\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        targets = torch.argmax(labels, dim=1)\n",
        "        correct += (preds == targets).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f\"{running_loss/total:.4f}\",\n",
        "            'acc': f\"{100.*correct/total:.2f}%\"\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "yit61HDuPbHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='Evaluating', leave=False)\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            targets = torch.argmax(labels, dim=1)\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{running_loss/total:.4f}\",\n",
        "                'acc': f\"{100.*correct/total:.2f}%\"\n",
        "            })\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "uexDa3hsP6tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def compute_class_weights(data_path: str, split: str = \"train\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute inverse-frequency class weights from dataset folders.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Base path containing 'train' and 'test' folders.\n",
        "        split (str): Split to compute weights from ('train' recommended).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Normalized class weights (higher for rarer classes).\n",
        "    \"\"\"\n",
        "    split_path = os.path.join(data_path, split)\n",
        "    classes = sorted(os.listdir(split_path))  # ensures consistent ordering\n",
        "\n",
        "    # Count samples per class\n",
        "    class_counts = []\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        count = len(os.listdir(cls_path))\n",
        "        class_counts.append(count)\n",
        "\n",
        "    # Compute inverse-frequency weights\n",
        "    class_counts = torch.tensor(class_counts, dtype=torch.float)\n",
        "    weights = 1.0 / class_counts\n",
        "    weights = weights / weights.sum() * len(classes)  # normalized for stability\n",
        "\n",
        "    print(f\"Class sample counts: {dict(zip(classes, class_counts.tolist()))}\")\n",
        "    print(f\"Computed class weights: {weights.tolist()}\")\n",
        "    return weights"
      ],
      "metadata": {
        "id": "wLG2VIlano5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, epochs=10, lr=1e-3, device='cuda', patience=3):\n",
        "    model = model.to(device)\n",
        "    data_path = \"/kaggle/input/emotion-detection-fer\"\n",
        "\n",
        "    # compute weights dynamically\n",
        "    class_weights = compute_class_weights(data_path, split=\"train\").to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'test_loss': [], 'test_acc': []\n",
        "    }\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.2f}%\")\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_loss = test_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_emotion_model.pth')\n",
        "            print(f\"Best model saved with accuracy: {best_acc:.2f}%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement for {patience_counter} epoch(s).\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {patience_counter} epochs with no improvement.\")\n",
        "            break\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "JxvEwoj2P9Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EmotionManipulator(num_classes=7)\n",
        "history = train_model(model, train_loader, test_loader, epochs=50, lr=3e-4, device=device)"
      ],
      "metadata": {
        "id": "7F3k_X67QFlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # --- Plot Loss ---\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(epochs, history['test_loss'], label='Test Loss', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Testing Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # --- Plot Accuracy ---\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['train_acc'], label='Train Accuracy', marker='o')\n",
        "    plt.plot(epochs, history['test_acc'], label='Test Accuracy', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training vs Testing Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call after training\n",
        "plot_training_curves(history)"
      ],
      "metadata": {
        "id": "NwyNb9PDY3kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_resnet = EmotionManipulatorResnet(num_classes=7)\n",
        "history_resnet = train_model(model_resnet, train_loader, test_loader, epochs=50, lr=3e-4, device=device)\n",
        "plot_training_curves(history_resnet)"
      ],
      "metadata": {
        "id": "z5U5emzVWr0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_residual = EmotionManipulatorResidual(num_classes=7)\n",
        "history_residual = train_model(model_residual, train_loader, test_loader, epochs=50, lr=3e-4, device=device)\n",
        "plot_training_curves(history_residual)"
      ],
      "metadata": {
        "id": "eqb5QBXuGHkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, device='cuda', class_names=None, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Plots the confusion matrix for a trained model on a given dataloader.\n",
        "    Works for multi-class (not multi-label) emotion classification.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            # For multi-class case\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = np.trace(cm) / np.sum(cm) * 100\n",
        "\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap='Blues', values_format='d', ax=plt.gca(), colorbar=False)\n",
        "    plt.title(f\"{title}\\nAccuracy: {acc:.2f}%\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FO2ZWQTvSMJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model, test_loader, 'cuda')"
      ],
      "metadata": {
        "id": "Mdfui9e2Sxlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_resnet, test_loader, 'cuda')"
      ],
      "metadata": {
        "id": "cB-lWKn_S5w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_residual, test_loader, 'cuda')"
      ],
      "metadata": {
        "id": "PcRoLREES8XN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}